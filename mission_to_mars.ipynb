{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission To Mars\n",
    "Jupyter notebook file to contain code for all web scraping and analysis of that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "# Confirm location of my chromdriver package\n",
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define executable path and create a 'browser' instance\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News\n",
    "Collect latest news title and paragraph from NASA Mars News Site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url, use splinter to visit the url, get the response object, and create the beautiful soup object.\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser.visit(url)\n",
    "time.sleep(2.5)\n",
    "response = browser.html\n",
    "news_soup = BeautifulSoup(response, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent article (i.e., the top article)\n",
    "try_counter = 0\n",
    "\n",
    "while try_counter <= 3:\n",
    "    try:\n",
    "        news_title = news_soup.find('div', class_='content_title').text\n",
    "        news_p = news_soup.find('div', class_='article_teaser_body').text\n",
    "        try_counter = 4\n",
    "    except:\n",
    "        try_counter = try_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTitle:\u001b[0m Space Samples Link NASA's Apollo 11 and Mars 2020\n",
      "\n",
      "\u001b[1mDescription:\u001b[0m While separated by half a century, NASA's Apollo 11 and Mars 2020 missions share the same historic goal: returning samples to Earth.\n"
     ]
    }
   ],
   "source": [
    "# Print variables\n",
    "print(\n",
    "    '\\033[1m' + 'Title:' + '\\033[0m' + f' {news_title}\\n\\n'\n",
    "    '\\033[1m' + 'Description:' + '\\033[0m' + f' {news_p}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image\n",
    "Use splinter to navigate the site and find the image url for the current Featured Mars Image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url (base to be used in final url path calculation and mars specific for page visit),\n",
    "# use splinter to visit the url, navigate the site, get the response object, \n",
    "# and create the beautiful soup object.\n",
    "base_url = 'https://www.jpl.nasa.gov'\n",
    "mars_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(mars_url)\n",
    "time.sleep(2.5)\n",
    "\n",
    "# Go one page in to get the featured image\n",
    "featured_image_xpath = '//*[@id=\"full_image\"]'\n",
    "browser.find_by_xpath(featured_image_xpath)[0].click()\n",
    "time.sleep(2.5)\n",
    "\n",
    "# Go one more page to get the high res image\n",
    "high_res_xpath = '//*[@id=\"fancybox-lock\"]/div/div[2]/div/div[1]/a[2]'\n",
    "browser.find_by_xpath(high_res_xpath)[0].click()\n",
    "time.sleep(2.5)\n",
    "\n",
    "response = browser.html\n",
    "image_soup = BeautifulSoup(response, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the featured image url. \n",
    "try_counter = 0\n",
    "while try_counter <= 3:\n",
    "    try:\n",
    "        # Find the featured image url. \n",
    "        featured_image_path = image_soup.find('figure', class_='lede').\\\n",
    "                                 find('a')['href']\n",
    "        featured_image_url = base_url + featured_image_path\n",
    "        try_counter = 4\n",
    "    except:\n",
    "        try_counter = try_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mURL: \u001b[0mhttps://www.jpl.nasa.gov/spaceimages/images/largesize/PIA17794_hires.jpg\n"
     ]
    }
   ],
   "source": [
    "# Print out url\n",
    "print('\\033[1m' + 'URL: ' + '\\033[0m' + featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Weather\n",
    "Scrape the latest Mars weather tweet from the Mars Weather twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMost recent tweet: \u001b[0mI’d say a plutonium-238 powered RTG qualifies the Curiosity and Mars2020 rovers as alternative fuel vehicles. You can explore these and other missions, rockets and more with JPL’s Spacecraft AR for IOS and Androhttps://www.jpl.nasa.gov/apps/\n"
     ]
    }
   ],
   "source": [
    "# Define url, use splinter to visit the url, get the response object, and create the beautiful soup object.\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(url)\n",
    "time.sleep(2.5)\n",
    "response = browser.html\n",
    "tweet_soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "# Find the most recent tweet (i.e., the top most tweet)\n",
    "try_counter = 0\n",
    "while try_counter <= 3:\n",
    "    try:\n",
    "        # Find the most recent tweet (i.e., the top most tweet)\n",
    "        mars_weather = tweet_soup.find('p', class_='TweetTextSize TweetTextSize--normal js-tweet-text tweet-text').\\\n",
    "                                        contents[0]\n",
    "        # Clean up the text by replacing \\n's with spaces and removing the url at the end.\n",
    "        mars_weather = mars_weather.replace('\\n', ' ')\n",
    "        try_counter = 4\n",
    "    except:\n",
    "        try_counter = try_counter + 1\n",
    "\n",
    "# Clean up the text by replacing \\n's with spaces and removing the url at the end.\n",
    "mars_weather = mars_weather.replace('\\n', ' ')\n",
    "print('\\033[1m' + 'Most recent tweet: ' + '\\033[0m' + mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts\n",
    "Scrape the table from the Mars Facts website for facts about the planet including Diameter, Mass, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url, use splinter to visit the url, get the response object, and create the beautiful soup object.\n",
    "url = 'https://space-facts.com/mars/'\n",
    "browser.visit(url)\n",
    "time.sleep(2.5)\n",
    "response = browser.html\n",
    "table_soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "# Find the table and use pandas to extract it into a dataframe.\n",
    "try_counter = 0\n",
    "while try_counter <= 3:\n",
    "    try:\n",
    "        # Find the table.\n",
    "        scraped_table = table_soup.find('table', id='tablepress-p-mars')\n",
    "        try_counter = 4\n",
    "    except:\n",
    "        try_counter = try_counter + 1\n",
    "\n",
    "# Extract table into a dataframe\n",
    "mars_facts_dataframe = pd.read_html(str(scraped_table))[0]\n",
    "\n",
    "# Remove the column headers and reset the index\n",
    "mars_facts_dataframe = mars_facts_dataframe.set_index(0)\n",
    "\n",
    "# Convert the table to an html string\n",
    "final_table = mars_facts_dataframe.to_html(header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Equatorial Diameter:</th>\n",
       "      <td>6,792 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polar Diameter:</th>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mass:</th>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moons:</th>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orbit Distance:</th>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orbit Period:</th>\n",
       "      <td>687 days (1.9 years)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surface Temperature:</th>\n",
       "      <td>-87 to -5 °C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Record:</th>\n",
       "      <td>2nd millennium BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recorded By:</th>\n",
       "      <td>Egyptian astronomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  1\n",
       "0                                                  \n",
       "Equatorial Diameter:                       6,792 km\n",
       "Polar Diameter:                            6,752 km\n",
       "Mass:                 6.39 × 10^23 kg (0.11 Earths)\n",
       "Moons:                          2 (Phobos & Deimos)\n",
       "Orbit Distance:            227,943,824 km (1.38 AU)\n",
       "Orbit Period:                  687 days (1.9 years)\n",
       "Surface Temperature:                   -87 to -5 °C\n",
       "First Record:                     2nd millennium BC\n",
       "Recorded By:                   Egyptian astronomers"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print table as a dataframe to demonstrate it was pulled correctly\n",
    "mars_facts_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres\n",
    "Scrape multiple images from the USGS Astrogeology site and store image urls in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base url and mars search url, define lists for hemisphere names and for image_urls,\n",
    "# iterate through each hemisphere type to find the src of the image via beautiful soup, and populate the dictionary.\n",
    "base_url = 'https://astrogeology.usgs.gov'\n",
    "mars_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "\n",
    "# Define lists of data from which to build our image urls.\n",
    "hemispheres_list = ['Cerberus Hemisphere', 'Schiaparelli Hemisphere', 'Syrtis Major Hemisphere', 'Valles Marineris Hemisphere']\n",
    "xpath_list = ['//*[@id=\"product-section\"]/div[2]/div[1]/a/img', '//*[@id=\"product-section\"]/div[2]/div[2]/a/img', '//*[@id=\"product-section\"]/div[2]/div[3]/a/img', '//*[@id=\"product-section\"]/div[2]/div[4]/a/img']\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Iteration\n",
    "for i in range(len(hemispheres_list)):\n",
    "    # Visit the page and navigate using splinter\n",
    "    browser.visit(mars_url)\n",
    "    time.sleep(2.5)\n",
    "    browser.find_by_xpath(xpath_list[i])[0].click()\n",
    "    time.sleep(2.5)\n",
    "    browser.find_by_xpath('//*[@id=\"wide-image-toggle\"]')[0].click()\n",
    "    time.sleep(2.5)\n",
    "    \n",
    "    # Store the response and create the beautiful soup object\n",
    "    response = browser.html\n",
    "    hemisphere_soup = BeautifulSoup(response, 'html.parser')\n",
    "    \n",
    "    try_counter = 0\n",
    "    while try_counter <= 3:\n",
    "        try:\n",
    "            # Grab the src from img tag (img_path) and build the correct url\n",
    "            img_path = hemisphere_soup.find('img', class_='wide-image')['src']\n",
    "            img_url = base_url + img_path\n",
    "            try_counter = 4\n",
    "        except:\n",
    "            try_counter = try_counter + 1\n",
    "    \n",
    "    # Store the dictionary entry temporarily and then append it to our list\n",
    "    temp_dict = {'title': hemispheres_list[i], 'img_url': img_url}   \n",
    "    hemisphere_image_urls.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere',\n",
       "  'img_url': 'https://astrogeology.usgs.gov/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print list of dictionaries\n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
